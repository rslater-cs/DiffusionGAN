{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.9/dist-packages (0.12.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from diffusers) (2.28.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.9/dist-packages (from diffusers) (6.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from diffusers) (0.14.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from diffusers) (1.23.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from diffusers) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from diffusers) (2022.10.31)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from diffusers) (9.2.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers) (23.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers) (4.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers) (5.4.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata->diffusers) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->diffusers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->diffusers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->diffusers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->diffusers) (2019.11.28)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.21.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: diffusers==0.12.1 in /usr/local/lib/python3.9/dist-packages (0.12.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from diffusers==0.12.1) (3.9.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from diffusers==0.12.1) (9.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from diffusers==0.12.1) (0.14.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from diffusers==0.12.1) (1.23.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from diffusers==0.12.1) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from diffusers==0.12.1) (2.28.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.9/dist-packages (from diffusers==0.12.1) (6.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.12.1) (4.4.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.12.1) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.12.1) (5.4.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.12.1) (2023.1.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.12.1) (23.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata->diffusers==0.12.1) (3.11.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->diffusers==0.12.1) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->diffusers==0.12.1) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->diffusers==0.12.1) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->diffusers==0.12.1) (2.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: open_clip_torch in /usr/local/lib/python3.9/dist-packages (2.19.0)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.9/dist-packages (from open_clip_torch) (0.14.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from open_clip_torch) (0.1.97)\n",
      "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from open_clip_torch) (1.12.1+cu116)\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.9/dist-packages (from open_clip_torch) (6.1.1)\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.9/dist-packages (from open_clip_torch) (0.9.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from open_clip_torch) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from open_clip_torch) (4.64.1)\n",
      "Requirement already satisfied: protobuf<4 in /usr/local/lib/python3.9/dist-packages (from open_clip_torch) (3.19.6)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from open_clip_torch) (0.13.1+cu116)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.0->open_clip_torch) (4.4.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy->open_clip_torch) (0.2.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->open_clip_torch) (2023.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->open_clip_torch) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->open_clip_torch) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->open_clip_torch) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->open_clip_torch) (3.9.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.9/dist-packages (from timm->open_clip_torch) (0.3.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->open_clip_torch) (9.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->open_clip_torch) (1.23.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->open_clip_torch) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->open_clip_torch) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets==2.10.1 in /usr/local/lib/python3.9/dist-packages (2.10.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1) (0.70.13)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1) (2023.1.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1) (0.3.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1) (0.14.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1) (2.28.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1) (5.4.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1) (10.0.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1) (3.8.3)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1) (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1) (1.23.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1) (1.5.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.10.1) (2.1.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.10.1) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.10.1) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.10.1) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.10.1) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.10.1) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.10.1) (18.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.10.1) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets==2.10.1) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets==2.10.1) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.10.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.10.1) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.10.1) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install diffusers\n",
    "%pip install transformers\n",
    "%pip install diffusers==0.12.1\n",
    "%pip install open_clip_torch\n",
    "%pip install datasets==2.10.1\n",
    "\n",
    "#cd ./.. # Move to root directory\n",
    "#ls -a # list directory and hidden items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "REF_yuHprSa1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import autocast\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import datasets\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler, LMSDiscreteScheduler, DDPMScheduler\n",
    "from diffusers.optimization import get_scheduler, get_cosine_schedule_with_warmup\n",
    "\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IA-VoQm3YW-5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "def decode_images(images):\n",
    "    images = (images / 2 + 0.5).clamp(0, 1)\n",
    "    images = images.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (images * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    \n",
    "    return pil_images\n",
    "\n",
    "def test_func(model, prompt, negative_prompt):\n",
    "    images = []\n",
    "    transform = transforms.Resize(100)\n",
    "    for i in range(0, 4):\n",
    "        for image in model.generate([prompt] * 8, [negative_prompt] * 8):\n",
    "            images.append(transform(image))\n",
    "\n",
    "    image_grid(images, 4, 8).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xlsKwQijWMpL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DiffusionModel:\n",
    "    def __init__(self, device, path = None):\n",
    "        self.device = device\n",
    "        \n",
    "        self.width = 512                         # default width of Stable Diffusion\n",
    "        self.height = 512                        # default height of Stable Diffusion\n",
    "\n",
    "        transformer_dir = os.getcwd() + \"/transformer\"\n",
    "        \n",
    "        # 1. Load the autoencoder model which will be used to decode the latents into image space. \n",
    "        self.vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2-1\", cache_dir=transformer_dir, subfolder=\"vae\", sample_size=1).to(self.device)\n",
    "\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(\"stabilityai/stable-diffusion-2-1\", cache_dir=transformer_dir, subfolder=\"tokenizer\")\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(\"stabilityai/stable-diffusion-2-1\", cache_dir=transformer_dir, subfolder=\"text_encoder\")\n",
    "\n",
    "        # 3. The UNet model for generating the latents.\n",
    "        if path:\n",
    "            self.unet = UNet2DConditionModel.from_pretrained(path).to(self.device)\n",
    "        else:\n",
    "            self.unet = UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2-1\", cache_dir=transformer_dir, subfolder=\"unet\").to(self.device)\n",
    "            \n",
    "        # 4. The scheduler for managing the denoising amount\n",
    "        self.scheduler = LMSDiscreteScheduler.from_pretrained(\"stabilityai/stable-diffusion-2-1\", cache_dir=transformer_dir, subfolder=\"scheduler\")\n",
    "        \n",
    "        self.generator = torch.manual_seed(1)\n",
    "        \n",
    "        self.noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
    "        \n",
    "        # default_lr = 5e-6\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.unet.parameters(),\n",
    "            lr=5e-6,\n",
    "            betas=(0.95, 0.999),\n",
    "            weight_decay=1e-6,\n",
    "            eps=1e-08,\n",
    "        )\n",
    "        \n",
    "    def prompts_to_embeddings(self, prompts):\n",
    "        # Embedding\n",
    "        text_input = self.tokenizer(prompts, padding=\"max_length\", max_length=self.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            text_embeddings = self.text_encoder(text_input.input_ids)\n",
    "            \n",
    "        return text_embeddings\n",
    "        \n",
    "    def prompts_to_query_embeddings(self, prompts, negative_prompts):\n",
    "        text_input = self.tokenizer(prompts, padding=\"max_length\", max_length=self.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_embeddings = self.text_encoder(text_input.input_ids)\n",
    "        \n",
    "        max_length = text_input.input_ids.shape[-1]\n",
    "        uncond_input = self.tokenizer(negative_prompts, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            uncond_embeddings = self.text_encoder(uncond_input.input_ids)\n",
    "        \n",
    "        return torch.cat([uncond_embeddings[0], text_embeddings[0]])\n",
    "    \n",
    "    def generate_latent_noise(self, num):\n",
    "        shape = (num, self.unet.in_channels, self.height // 8, self.width // 8)\n",
    "        \n",
    "        latents = torch.randn(shape, generator=self.generator)\n",
    "        \n",
    "        latents = latents * self.scheduler.init_noise_sigma\n",
    "        \n",
    "        return latents\n",
    "    \n",
    "    def generate_step(self, step_index, latents, text_embeddings, guidance_scale):\n",
    "        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "        latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "        latent_model_input = self.scheduler.scale_model_input(latent_model_input, step_index)\n",
    "\n",
    "        # predict the noise residual\n",
    "        with torch.no_grad():\n",
    "            noise_pred = self.unet(latent_model_input, step_index, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "        # perform guidance\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        # compute the previous noisy sample x_t -> x_t-1\n",
    "        return self.scheduler.step(noise_pred, step_index, latents).prev_sample\n",
    "            \n",
    "    def generate_raw(self, latents, text_embeddings):\n",
    "        guidance_scale = 7.5                # Scale for classifier-free guidance\n",
    "\n",
    "        latents = latents.to(self.device)\n",
    "        \n",
    "        text_embeddings = text_embeddings.to(self.device)\n",
    "        \n",
    "        for step_index in tqdm(self.scheduler.timesteps):\n",
    "            latents = self.generate_step(step_index, latents, text_embeddings, guidance_scale)\n",
    "            \n",
    "        # scale and decode the image latents with vae\n",
    "        latents = 1 / 0.18215 * latents\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image = self.vae.decode(latents).sample\n",
    "            \n",
    "        return image\n",
    "    \n",
    "    def generate(self, prompts, negative_prompts, steps):\n",
    "        self.scheduler.set_timesteps(steps)\n",
    "        \n",
    "        latents = self.generate_latent_noise(len(prompts))\n",
    "    \n",
    "        text_embeddings = self.prompts_to_query_embeddings(prompts, negative_prompts)\n",
    "    \n",
    "        return decode_images(self.generate_raw(latents, text_embeddings))\n",
    "    \n",
    "    def show_image(self, latents):\n",
    "        with torch.no_grad():\n",
    "            image = self.vae.decode(latents).sample\n",
    "            \n",
    "        image_grid([decode_image(image)], 1, 1).show()\n",
    "            \n",
    "    def train_step(self, batch, lr_scheduler):\n",
    "        images = batch[\"image\"].to(self.device)\n",
    "        \n",
    "        # Convert image to latents\n",
    "        with torch.no_grad():\n",
    "            image_latents = self.vae.encode(images.float().cuda()).latent_dist.sample()\n",
    "\n",
    "        # Generate image noise\n",
    "        noise = torch.randn(image_latents.shape).to(self.device)\n",
    "\n",
    "        # Generate random timesteps\n",
    "        bsz = image_latents.shape[0]\n",
    "        timesteps = torch.randint(0, self.noise_scheduler.config.num_train_timesteps, (bsz,), device=self.device).long()\n",
    "\n",
    "        # Generate noisy versions of images based on timesteps\n",
    "        noise_latents = self.noise_scheduler.add_noise(image_latents, noise, timesteps)\n",
    "\n",
    "        text_embeddings = self.prompts_to_embeddings(batch[\"text\"])\n",
    "\n",
    "        predicted_latents = self.unet(noise_latents, timesteps, encoder_hidden_states=text_embeddings.last_hidden_state.cuda()).sample\n",
    "        \n",
    "        #self.show_image(image_latents)\n",
    "        #self.show_image(noise)\n",
    "        #self.show_image(noise_latents)\n",
    "        #self.show_image(predicted_latents)\n",
    "        \n",
    "        loss = mse_loss(predicted_latents, noise, reduction=\"mean\")\n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "    def train(self, dataset, epochs, batch_size):\n",
    "        train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        lr_scheduler = get_scheduler(\n",
    "            \"linear\",\n",
    "            optimizer=self.optimizer,\n",
    "            num_warmup_steps=500,\n",
    "            num_training_steps=(len(train_dataloader) * epochs),\n",
    "        )\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            for batch in tqdm(train_dataloader):\n",
    "                # Change the image to be (3, width, height) format\n",
    "                #image = entry[\"image\"].permute(2, 0, 1)\n",
    "                self.train_step(batch, lr_scheduler)\n",
    "            \n",
    "    def save(self, path):\n",
    "        self.unet.save_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual dataset if datasets module isn't able to download anything\n",
    "class HAMDataset:\n",
    "    def __init__(self, filename):\n",
    "        self.entries = []\n",
    "        \n",
    "        with open(filename, newline='') as csvfile:\n",
    "            csv_reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "            \n",
    "            csv_reader.__next__()\n",
    "            \n",
    "            for _, file_name, dx, dx_type, age, sex, localization in csv_reader:\n",
    "                self.entries.append({\"file_name\": file_name, \"dx\": dx, \"dx_type\": dx_type, \"age\": age, \"sex\": sex, \"localization\": localization})\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.entries[idx]\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(\"download/HAM10000_images_part_1/\" + entry[\"file_name\"] + \".jpg\")\n",
    "        except Exception as e:\n",
    "            img = Image.open(\"download/HAM10000_images_part_2/\" + entry[\"file_name\"] + \".jpg\")\n",
    "            \n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        \n",
    "        entry[\"image\"] = convert_tensor(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            entry = self.transform(entry)\n",
    "        \n",
    "        return entry\n",
    "    \n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metadata(Dataset):\n",
    "    def __init__(self, root: Path, transform: transforms.Compose) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.transform = transform\n",
    "        data_path = root / 'all_encoded.csv'\n",
    "        attn_path = root / 'all_attention.csv'\n",
    "        self.labels = pd.read_csv(data_path)\n",
    "\n",
    "        self.attn = pd.read_csv(attn_path)\n",
    "\n",
    "        self.attn = self.attn.drop('image', axis=1)\n",
    "\n",
    "        image_names = self.labels['image']\n",
    "\n",
    "        for i in range(len(image_names)):\n",
    "            image_names.iloc[i] = root / 'images' / image_names.iloc[i]\n",
    "\n",
    "        self.image_paths = image_names\n",
    "        # Remove all rows where there are no labels at all, this step is now redundant due to \n",
    "        # the revised encoding process\n",
    "        samples = self.attn.sum(axis=1) > 0\n",
    "        self.image_paths = self.image_paths[samples]\n",
    "\n",
    "        self.labels = self.labels[samples]\n",
    "\n",
    "        self.attn = self.attn[samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths.iloc[index])\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return (image, \n",
    "                self.labels['disease'].iloc[index], \n",
    "                self.labels['sex'].iloc[index], \n",
    "                self.labels['age'].iloc[index], \n",
    "                self.labels['anatom_site'].iloc[index],\n",
    "                self.labels['benign_malignant'].iloc[index],\n",
    "                torch.tensor(self.attn.iloc[index].values).type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextPrompt(Metadata):\n",
    "    disease_to_str = {\n",
    "        0:'type A',\n",
    "        1:'type B',\n",
    "        2:'type C',\n",
    "        3:'type D',\n",
    "        4:'type E',\n",
    "        5:'type F',\n",
    "        6:'type G',\n",
    "        7:'type H',\n",
    "        8:'type I',\n",
    "        9:'type J'\n",
    "    }\n",
    "\n",
    "    sex_to_str = {\n",
    "        0:'male',\n",
    "        1:'female'\n",
    "    }\n",
    "\n",
    "    age_to_str = lambda s, x: str(int(x*5))\n",
    "\n",
    "    site_to_str = {\n",
    "        0:'head/neck',\n",
    "        1:'upper extremity',\n",
    "        2:'lower extremity',\n",
    "        3:'torso',\n",
    "        4:'palms/soles',\n",
    "        5:'oral/genital',\n",
    "        6:'anterior torso',\n",
    "        7:'posterior torso',\n",
    "        8:'lateral torso',\n",
    "\n",
    "    }\n",
    "\n",
    "    bm_to_str = {\n",
    "        0:'benign',\n",
    "        1:'malignant'\n",
    "    }\n",
    "\n",
    "    def __init__(self, root: Path, transform: transforms.Compose) -> None:\n",
    "        super().__init__(root, transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths.iloc[index])\n",
    "        image = self.transform(image)\n",
    "        attn = self.attn.iloc[index]\n",
    "        labels = self.labels.iloc[index]\n",
    "\n",
    "        prompt = 'An image of '\n",
    "        # place bening/malignant into string\n",
    "        if attn['4'] == 1:\n",
    "            prompt += f'{self.bm_to_str[labels[\"benign_malignant\"]]} '\n",
    "            if attn['0'] == 0:\n",
    "                prompt += 'cancer '\n",
    "                \n",
    "        # place disease into string\n",
    "        if attn['0'] == 1:\n",
    "            prompt += f'{self.disease_to_str[labels[\"disease\"]]} '\n",
    "            \n",
    "        # prepare if we have metadata\n",
    "        if attn['1'] == 1 or attn['2'] == 1 or attn['3'] == 1:\n",
    "            prompt += 'on '\n",
    "            \n",
    "        # place site into string\n",
    "        if attn['3'] == 1:\n",
    "            prompt += f'the {self.site_to_str[labels[\"anatom_site\"]]} of '\n",
    "            \n",
    "        # prepare if we have metadata\n",
    "        if attn['1'] == 1 or attn['2'] == 1 or attn['3'] == 1:\n",
    "            prompt += 'a '\n",
    "            \n",
    "        # place age into string\n",
    "        if attn['2'] == 1:\n",
    "            prompt += f'{self.age_to_str(labels[\"age\"])} year old '\n",
    "        # place sex into string\n",
    "        \n",
    "        if attn['1'] == 1:\n",
    "            prompt += f'{self.sex_to_str[labels[\"sex\"]]}'\n",
    "            \n",
    "        data = {'image':image, 'text':prompt}\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPrompt(Dataset):\n",
    "    disease_to_str = {\n",
    "        0:'type A',\n",
    "        1:'type B',\n",
    "        2:'type C',\n",
    "        3:'type D',\n",
    "        4:'type E',\n",
    "        5:'type F',\n",
    "        6:'type G',\n",
    "        7:'type H',\n",
    "        8:'type I',\n",
    "        9:'type J'\n",
    "    }\n",
    "\n",
    "    sex_to_str = {\n",
    "        0:'male',\n",
    "        1:'female'\n",
    "    }\n",
    "\n",
    "    age_to_str = lambda s, x: str(int(x*5))\n",
    "\n",
    "    site_to_str = {\n",
    "        0:'head/neck',\n",
    "        1:'upper extremity',\n",
    "        2:'lower extremity',\n",
    "        3:'torso',\n",
    "        4:'palms/soles',\n",
    "        5:'oral/genital',\n",
    "        6:'anterior torso',\n",
    "        7:'posterior torso',\n",
    "        8:'lateral torso',\n",
    "\n",
    "    }\n",
    "\n",
    "    bm_to_str = {\n",
    "        0:'benign',\n",
    "        1:'malignant'\n",
    "    }\n",
    "\n",
    "    def __init__(self, root: Path, num_images: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        data_path = root / 'all_encoded.csv'\n",
    "        attn_path = root / 'all_attention.csv'\n",
    "        labels = pd.read_csv(data_path)\n",
    "\n",
    "        labels = labels['disease']\n",
    "\n",
    "        attn = pd.read_csv(attn_path)\n",
    "\n",
    "        attn = attn.drop('image', axis=1)\n",
    "\n",
    "        if not os.path.exists(str(root) + '/fake_images'):\n",
    "            os.mkdir(str(root) + '/fake_images')\n",
    "\n",
    "        data = {'image_dir':[], 'text':[]}\n",
    "\n",
    "        new_labels = {'image':[],'sex':[],'age':[],'anatom_site':[],'benign_malignant':[],'disease':[]}\n",
    "\n",
    "        for i in range(num_images):\n",
    "            data['image_dir'].append(str(root / 'fake_images' / f'ISIC_Fake_{i}.jpg'))\n",
    "\n",
    "            new_labels['image'].append(f'ISIC_Fake_{i}.jpg')\n",
    "\n",
    "        distribution = labels.value_counts().sort_index()\n",
    "\n",
    "        mean = distribution.mean()\n",
    "\n",
    "        distribution = mean-distribution\n",
    "        distribution[distribution < 0] = 0\n",
    "        tot = distribution.sum()\n",
    "\n",
    "        distribution = distribution / tot\n",
    "        print(distribution)\n",
    "\n",
    "        distribution = distribution * num_images\n",
    "        distribution = distribution.round()\n",
    "\n",
    "        distribution = distribution.astype(int)\n",
    "\n",
    "        if distribution.sum() > num_images:\n",
    "            distribution.iloc[2] += num_images - distribution.sum()\n",
    "\n",
    "        distribution = list(distribution.values)\n",
    "\n",
    "        for i, num_samples in enumerate(distribution):\n",
    "            for j in range(num_samples):\n",
    "                new_labels['disease'].append(i)\n",
    "\n",
    "                disease = self.disease_to_str[i]\n",
    "\n",
    "                age = random.randint(0, 18)\n",
    "                new_labels['age'].append(age)\n",
    "\n",
    "                age = self.age_to_str(age)\n",
    "\n",
    "                sex = random.randint(0, 1)\n",
    "                new_labels['sex'].append(sex)\n",
    "\n",
    "                sex = self.sex_to_str[sex]\n",
    "\n",
    "                site = random.randint(0, 8)\n",
    "                new_labels['anatom_site'].append(site)\n",
    "\n",
    "                site = self.site_to_str[site]\n",
    "\n",
    "                bm = random.randint(0, 1)\n",
    "                new_labels['benign_malignant'].append(bm)\n",
    "\n",
    "                bm = self.bm_to_str[bm]\n",
    "\n",
    "                prompt = f'{bm} {disease} {site} {age} {sex}'\n",
    "                data['text'].append(prompt)\n",
    "\n",
    "        self.data = data\n",
    "        frame = pd.DataFrame(new_labels)\n",
    "\n",
    "        frame.to_csv(root / 'fake_labels.csv')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['text'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data['text'][index], self.data['image_dir'][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset1():\n",
    "    resize_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(512),\n",
    "        transforms.CenterCrop((512,512)),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    dataset = ImageTextPrompt(Path(\"new_dataset/\"), resize_transform)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def dataset2():\n",
    "    dataset = HAMDataset(\"ham10000/HAM10000_metadata.csv\")\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(512), # Makes sense for these images in particular\n",
    "        transforms.RandomHorizontalFlip()\n",
    "    ])\n",
    "\n",
    "    def make_prompt(entry):\n",
    "        dx = entry[\"dx\"]\n",
    "        dx_type = entry[\"dx_type\"]\n",
    "        age = entry[\"age\"]\n",
    "        sex = entry[\"sex\"]\n",
    "        position = entry[\"localization\"]\n",
    "\n",
    "        return f\"An image of {dx} skin cancer of type {dx_type} located at {position} on a {sex} of age {age}\"\n",
    "\n",
    "    def transform_images(entry):\n",
    "        new_entry = {}\n",
    "        new_entry[\"image\"] = transform(entry[\"image\"])\n",
    "        new_entry[\"text\"] = make_prompt(entry)\n",
    "        return new_entry\n",
    "\n",
    "    dataset.set_transform(transform_images)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def dataset3():\n",
    "    dataset_dir = os.getcwd() + \"/marmal\"\n",
    "    \n",
    "    # datsets that automatically downloads datasets\n",
    "    dataset = datasets.load_dataset(\"marmal88/skin_cancer\", cache_dir=dataset_dir).with_format(\"torch\").cast_column(\"image\", datasets.Image(decode=True))[\"train\"]\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(512), # Makes sense for these images in particular\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    def transform_images(entries):\n",
    "        new_entries = {}\n",
    "        new_entries[\"image\"] = [transform(entry.convert(\"RGB\")) for entry in entries[\"image\"]]\n",
    "        new_entries[\"text\"] = []\n",
    "        \n",
    "        for i in range(len(entries[\"dx\"])):\n",
    "            dx = entries[\"dx\"][i]\n",
    "            dx_type = entries[\"dx_type\"][i]\n",
    "            age = entries[\"age\"][i]\n",
    "            sex = entries[\"sex\"][i]\n",
    "            position = entries[\"localization\"][i]\n",
    "\n",
    "            new_entries[\"text\"].append(f\"An image of {dx} skin cancer of type {dx_type}. The skin cancer is located at {position} on a {sex} of age {age}\")\n",
    "            \n",
    "        return new_entries\n",
    "\n",
    "    dataset.set_transform(transform_images)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mZpvyVT1Y6wq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_dataset():\n",
    "    dataset = dataset1()\n",
    "    \n",
    "    model = DiffusionModel(torch_device)\n",
    "    \n",
    "    a6000 = 5\n",
    "    a100 = 13\n",
    "    \n",
    "    model.train(dataset, 1, a100)\n",
    "    \n",
    "    model.save(\"save\")\n",
    "    \n",
    "    test_func(model, \"dog\", \"\")\n",
    "    test_func(model, \"actinic_keratoses skin cancer of type histo located on the foot of a male of age 50.0\", \"deformed body features disfigured blurry dot pattern hairs lines\")\n",
    "    test_func(model, \"skin cancer\", \"deformed body features disfigured blurry dot pattern hairs lines\")\n",
    "    test_func(model, \"actinic_keratoses\", \"deformed body features disfigured blurry dot pattern hairs lines tile bacteria root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(batch_size, image_num):\n",
    "    model = DiffusionModel(torch_device, \"save\")\n",
    "    \n",
    "    dataset = TextPrompt(Path(\"new_dataset\"), image_num)\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    negative_prompt = \"deformed body features disfigured blurry line hair dot pattern repeat\"\n",
    "    negative_prompts = [negative_prompt] * batch_size\n",
    "\n",
    "    transform = transforms.Resize(224)\n",
    "    \n",
    "    for prompts, file_paths in iter(dataloader):\n",
    "        images = model.generate(prompts, negative_prompts, 100)\n",
    "        \n",
    "        for i in range(0, batch_size):\n",
    "            transform(images[i]).save(file_paths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.000000\n",
      "1    0.000000\n",
      "2    0.072031\n",
      "3    0.141779\n",
      "4    0.152603\n",
      "5    0.159181\n",
      "6    0.158987\n",
      "7    0.149605\n",
      "8    0.165814\n",
      "9    0.000000\n",
      "Name: disease, dtype: float64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c539878745496b8df2361acf904068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fbaefca2280430fbc5c3c35ba5d4d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2f46a7bd374e5888eb90c02bbb422e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e9b556c1d342dfaef5a10eb6535726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc47f5bbe84d40289ca6088b57800fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae295f7d4d43407ab4a582d6a3f1595e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b07f994891449bb13a39386f1e6472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_dataset()\n",
    "generate_images(24, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
